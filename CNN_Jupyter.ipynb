{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('tf': conda)",
   "metadata": {
    "interpreter": {
     "hash": "5218963ac3e58095a706b711472ce34526a4d3c94a575023db5e17b4eff23b13"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Activation, Flatten, Conv1D, Dropout, MaxPooling1D, Convolution1D, UpSampling1D\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = 60001 \n",
    "n_sensors = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "###model###\n",
    "###########\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 8, kernel_size = 11, input_shape = (data_features, n_sensors)))\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "model.add(Convolution1D(filters = 16, kernel_size = 9))\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "model.add(Convolution1D(filters = 16, kernel_size = 7))\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "model.add(Convolution1D(filters = 32, kernel_size = 7))\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "model.add(Convolution1D(filters = 32, kernel_size = 5))\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "model.add(Convolution1D(filters = 64, kernel_size = 5))\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "model.add(Convolution1D(filters = 64, kernel_size = 3))\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "###########\n",
    "#optimizer#\n",
    "###########\n",
    "\n",
    "#optimizer = keras.optimizers.Adadelta(lr = 1.0, rho = 0.95, epsilon = None, decay = 1e-6)\t\n",
    "optimizer = optimizers.SGD(lr = 0.0001, momentum = 0.0, decay=0.0, nesterov=False)\n",
    "#optimizer = keras.optimizers.Nadam(lr = 0.002, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, schedule_decay = 0.004)\n",
    "#optimizer = keras.optimizers.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999, epsilon = None, decay = 0.0, amsgrad = False)\n",
    "model.compile(loss = 'mse', optimizer = optimizer, metrics = ['accuracy'])\n",
    "\n",
    "#early stop\n",
    "earlyStopping = EarlyStopping(monitor = 'loss', patience = 20, verbose = 1, mode = 'max')\n",
    "\n",
    "# reduce learning rate when accuracy keep stable\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor = 'val_acc', factor = 0.1,patience = 7, verbose = 1, min_delta = 1e-4,mode = 'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/2\n",
      "36/36 [==============================] - 15s 423ms/step - loss: 0.0909 - accuracy: 0.0000e+00 - val_loss: 0.0981 - val_accuracy: 0.0112\n",
      "Epoch 2/2\n",
      "36/36 [==============================] - 14s 391ms/step - loss: 0.0449 - accuracy: 0.0000e+00 - val_loss: 0.0972 - val_accuracy: 0.0112\n",
      "Epoch 1/2\n",
      "36/36 [==============================] - 15s 414ms/step - loss: 0.0920 - accuracy: 0.0056 - val_loss: 0.1324 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "36/36 [==============================] - 14s 403ms/step - loss: 0.0510 - accuracy: 0.0056 - val_loss: 0.0821 - val_accuracy: 0.0000e+00\n",
      "Epoch 1/2\n",
      "36/36 [==============================] - 14s 401ms/step - loss: 0.0811 - accuracy: 0.0028 - val_loss: 0.0795 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "36/36 [==============================] - 14s 400ms/step - loss: 0.0431 - accuracy: 0.0028 - val_loss: 0.0837 - val_accuracy: 0.0000e+00\n",
      "Epoch 1/2\n",
      "36/36 [==============================] - 15s 410ms/step - loss: 0.0896 - accuracy: 0.0000e+00 - val_loss: 0.0821 - val_accuracy: 0.0112\n",
      "Epoch 2/2\n",
      "36/36 [==============================] - 15s 412ms/step - loss: 0.0454 - accuracy: 0.0028 - val_loss: 0.0852 - val_accuracy: 0.0112\n",
      "Epoch 1/2\n",
      "36/36 [==============================] - 15s 410ms/step - loss: 0.0773 - accuracy: 0.0000e+00 - val_loss: 0.1097 - val_accuracy: 0.0112\n",
      "Epoch 2/2\n",
      "36/36 [==============================] - 15s 416ms/step - loss: 0.0416 - accuracy: 0.0028 - val_loss: 0.1053 - val_accuracy: 0.0112\n",
      "Epoch 1/2\n",
      "36/36 [==============================] - 15s 405ms/step - loss: 0.0904 - accuracy: 0.0056 - val_loss: 0.1193 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "36/36 [==============================] - 15s 417ms/step - loss: 0.0507 - accuracy: 0.0056 - val_loss: 0.0871 - val_accuracy: 0.0000e+00\n",
      "Epoch 1/2\n",
      "36/36 [==============================] - 15s 409ms/step - loss: 0.0879 - accuracy: 0.0028 - val_loss: 0.1517 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "36/36 [==============================] - 15s 410ms/step - loss: 0.0488 - accuracy: 0.0056 - val_loss: 0.0909 - val_accuracy: 0.0000e+00\n",
      "Epoch 1/2\n",
      "36/36 [==============================] - 15s 411ms/step - loss: 0.0792 - accuracy: 0.0000e+00 - val_loss: 0.0923 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "36/36 [==============================] - 15s 414ms/step - loss: 0.0446 - accuracy: 0.0028 - val_loss: 0.0798 - val_accuracy: 0.0000e+00\n",
      "Epoch 1/2\n",
      "36/36 [==============================] - 15s 409ms/step - loss: 0.0871 - accuracy: 0.0000e+00 - val_loss: 0.0775 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "36/36 [==============================] - 15s 411ms/step - loss: 0.0534 - accuracy: 0.0000e+00 - val_loss: 0.0837 - val_accuracy: 0.0000e+00\n",
      "Epoch 1/2\n",
      "36/36 [==============================] - 15s 415ms/step - loss: 0.0730 - accuracy: 0.0028 - val_loss: 0.0700 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "36/36 [==============================] - 15s 430ms/step - loss: 0.0475 - accuracy: 0.0028 - val_loss: 0.0766 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:From C:\\Users\\12396\\miniconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\12396\\miniconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: cnn_model\\assets\n"
     ]
    }
   ],
   "source": [
    "#----load training data----\n",
    "\n",
    "label = np.array(pd.read_csv('data/train.csv'))\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    #initialize training dataset\n",
    "    x = np.zeros((443, data_features, n_sensors))\n",
    "    y_l = []\n",
    "\n",
    "    #split dataset to 10 set, apply 2 epoch train on 1 set \n",
    "    count = 0\n",
    "    for j in range(443):\n",
    "        #read sensor readings and fill the missing reading with zero\n",
    "        file = pd.read_csv('data/train/' + str(label[i*443 + j][0]) +'.csv').fillna(0)\n",
    "        file_i = np.array(file)\n",
    "\n",
    "        #Normalization\n",
    "        nor = Normalizer()\n",
    "        file_s = nor.fit_transform(file_i)\n",
    "        \n",
    "        x[count] = file_s\n",
    "        y_l.append(label[i*443 + j][1])\n",
    "        count+=1\n",
    "\n",
    "    y = np.array(y_l)\n",
    "    y_max = np.max(y)\n",
    "    y_min = np.min(y)\n",
    "    y = (y - y_min)/(y_max - y_min)\n",
    "\n",
    "    h = model.fit(x, y, epochs = 2, batch_size = 10, verbose = 1, validation_split = 0.2)\n",
    "\n",
    "model.save(\"cnn_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MAE:\n970180855.4164661\n"
     ]
    }
   ],
   "source": [
    "\n",
    "############\n",
    "###evalue###\n",
    "############\n",
    "l_max = np.max(label)\n",
    "l_min = np.min(label)\n",
    "\n",
    "result = np.zeros(len(label))\n",
    "y_t = np.column_stack((label[:,0], result.T))\n",
    "\n",
    "for i in range(4431):\n",
    "    #load test data\n",
    "    file = pd.read_csv('data/train/' + str(int(y_t[i][0])) +'.csv').fillna(0)\n",
    "    file_i = np.matrix(file)\n",
    "\n",
    "    #standardization\n",
    "    nor = Normalizer()\n",
    "    x_test = nor.fit_transform(file_i)\n",
    "    x_test = np.expand_dims(x_test, 0)\n",
    "    #predict\n",
    "    test_softmax_output = model.predict(x_test)\t\t\n",
    "    test_predictions = test_softmax_output[0][0]\n",
    "\n",
    "    y_t[i][1] = test_predictions*(l_max - l_min) + y_min\n",
    "\n",
    "print('MAE:')\n",
    "print(mean_absolute_error(label[:,1], y_t[:,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "###predict###\n",
    "#############\n",
    "\n",
    "y_p = np.array(pd.read_csv('data/sample_submission.csv'))\n",
    "\n",
    "for i in range(4520):\n",
    "    #load test data\n",
    "    file = pd.read_csv('data/test/' + str(int(y_p[i][0])) +'.csv').fillna(0)\n",
    "    file_i = np.matrix(file)\n",
    "\n",
    "    #standardization\n",
    "    nor = Normalizer()\n",
    "    x_p = nor.fit_transform(file_i)\n",
    "    x_p = np.expand_dims(x_p, 0)\n",
    "    #predict\n",
    "    p_softmax_output = model.predict(x_p)\t\t\n",
    "    p_predictions = p_softmax_output[0][0]\n",
    "\n",
    "    y_p[i][1] = p_predictions*(l_max - l_min) + y_min\n",
    "\n",
    "sub = pd.DataFrame(y_p, columns = ['segment_id', 'time_to_eruption'])\n",
    "sub.to_csv('submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}